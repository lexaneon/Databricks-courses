{"cells":[{"cell_type":"markdown","source":["# Databricks ML Quickstart: Model Training\n\nThis notebook provides a quick overview of machine learning model training on Databricks. To train models, you can use libraries like scikit-learn that are preinstalled on the Databricks Runtime for Machine Learning. In addition, you can use MLflow to track the trained models, and Hyperopt with SparkTrials to scale hyperparameter tuning.\n\nThis tutorial covers:\n- Part 1: Training a simple classification model with MLflow tracking\n- Part 2: Hyperparameter tuning a better performing model with Hyperopt\n\nFor more details on productionizing machine learning on Databricks including model lifecycle management and model inference, see the ML End to End Example ([AWS](https://docs.databricks.com/applications/mlflow/end-to-end-example.html)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/end-to-end-example)).\n\n### Requirements\n- Cluster running Databricks Runtime 10.1 ML or above"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"528c6d2b-3c5a-4236-ac61-fa8cc8d4d323","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import mlflow\nimport numpy as np\nimport pandas as pd\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.model_selection\nimport sklearn.ensemble\n\nfrom hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\nfrom hyperopt.pyll import scope"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cec61d6d-ea1a-4d2f-9ee6-625393a24aa5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Pre-requisite**:\n\n1. As an account-level admin or metastore admin, create catalog 'ml': \n\n   ```\n   CREATE CATALOG ml;\n   ```\n\n2. Grant the `account users` group access to the catalog and schema, and the ability to create tables and views in the schema:\n\n   ```\n   GRANT USAGE on CATALOG ml to `account users`;\n\n   GRANT USAGE, CREATE on SCHEMA ml.default to `account users`;\n   ```\n\nNow, any account-level user can run the commands in this notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5f92896e-6da3-4622-94de-5b07d3cd594b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSHOW GRANTS ON CATALOG ml;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"a637d7b3-2c45-4cfc-a114-84b4b7f26ead","inputWidgets":{},"title":"Setup: Show your access level on the ml catalog"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSHOW GRANTS ON SCHEMA ml.default;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"82c4c850-61e8-4b08-9cc3-9b01d3c13b1a","inputWidgets":{},"title":"Setup: Show your access level on the ml.default schema"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# This example uses publicly-available data that we clean up and copy to Unity Catalog so that others can use the same dataset\n\n# Read the data from CSV files\nwhite_wine = spark.read.csv(\"dbfs:/databricks-datasets/wine-quality/winequality-white.csv\", sep=';', header=True)\nred_wine = spark.read.csv(\"dbfs:/databricks-datasets/wine-quality/winequality-red.csv\", sep=';', header=True)\n\n# To clean up the data, remove the spaces from the column names, since parquet doesn't allow them\nfor c in white_wine.columns:\n    white_wine = white_wine.withColumnRenamed(c, c.replace(\" \", \"_\"))\nfor c in red_wine.columns:\n    red_wine = red_wine.withColumnRenamed(c, c.replace(\" \", \"_\"))\n\n# Write to tables in Unity Catalog\nspark.sql(\"drop table if exists ml.default.white_wine\")\nspark.sql(\"drop table if exists ml.default.red_wine\")\nwhite_wine.write.saveAsTable(\"ml.default.white_wine\")\nred_wine.write.saveAsTable(\"ml.default.red_wine\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"ef33c915-9b59-42a3-9ab9-1fc42f0d57de","inputWidgets":{},"title":"Setup: Write data to Unity Catalog"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Load data from Unity Catalog"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2f67ffe-ad1b-49a1-a7cf-603daa8c9890","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Load data from Unity Catalog as Pandas dataframes\nwhite_wine = spark.read.table(\"ml.default.white_wine\").toPandas()\nred_wine = spark.read.table(\"ml.default.red_wine\").toPandas()\n\n# Add Boolean fields for red and white wine\nwhite_wine['is_red'] = 0.0\nred_wine['is_red'] = 1.0\ndata_df = pd.concat([white_wine, red_wine], axis=0)\n\n# Define classification labels based on the wine quality\ndata_labels = data_df['quality'].astype('int') >= 7\ndata_df = data_df.drop(['quality'], axis=1)\n\n# Split 80/20 train-test\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n  data_df,\n  data_labels,\n  test_size=0.2,\n  random_state=1\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51fd8a9f-bef3-4fbd-90ce-8531f5f71205","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Part 1. Train a classification model"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8123df40-67fa-43d0-97c1-6f608c9f7d61","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### MLflow Tracking\n[MLflow tracking](https://www.mlflow.org/docs/latest/tracking.html) allows you to organize your machine learning training code, parameters, and models. \n\nYou can enable automatic MLflow tracking by using [*autologging*](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f3441df0-a7fe-4d6d-bf62-46184cf6ac2c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Enable MLflow autologging for this notebook\nmlflow.autolog()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18e0381e-7c02-4e57-b29b-127a7585992b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Next, train a classifier within the context of an MLflow run, which automatically logs the trained model and many associated metrics and parameters. \n\nYou can supplement the logging with additional metrics such as the model's AUC score on the test dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"88913160-61c4-4cbf-8f2d-235e86fa8768","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["with mlflow.start_run(run_name='gradient_boost') as run:\n  model = sklearn.ensemble.GradientBoostingClassifier(random_state=0)\n  \n  # Models, parameters, and training metrics are tracked automatically\n  model.fit(X_train, y_train)\n\n  predicted_probs = model.predict_proba(X_test)\n  roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n  \n  # The AUC score on test data is not automatically logged, so log it manually\n  mlflow.log_metric(\"test_auc\", roc_auc)\n  print(\"Test AUC of: {}\".format(roc_auc))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70fefb47-9af8-49c8-932d-49a0727c1428","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["If you aren't happy with the performance of this model, train another model with different hyperparameters."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"06eadee7-a786-4a6a-be73-c70a7c18c0a0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Start a new run and assign a run_name for future reference\nwith mlflow.start_run(run_name='gradient_boost') as run:\n  model_2 = sklearn.ensemble.GradientBoostingClassifier(\n    random_state=0, \n    \n    # Try a new parameter setting for n_estimators\n    n_estimators=200,\n  )\n  model_2.fit(X_train, y_train)\n\n  predicted_probs = model_2.predict_proba(X_test)\n  roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n  mlflow.log_metric(\"test_auc\", roc_auc)\n  print(\"Test AUC of: {}\".format(roc_auc))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"85fa6e22-56ab-44da-89c0-e77f883c5fcd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### View MLflow runs\nTo view the logged training runs, click the **Experiment** icon at the upper right of the notebook to display the experiment sidebar. If necessary, click the refresh icon to fetch and monitor the latest runs. \n\n<img width=\"350\" src=\"https://docs.databricks.com/_static/images/mlflow/quickstart/experiment-sidebar-icons.png\"/>\n\nTo display the more detailed MLflow experiment page, click the experiment page icon. ([AWS](https://docs.databricks.com/applications/mlflow/tracking.html#notebook-experiments)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/tracking#notebook-experiments)). This page allows you to compare runs and view details for specific runs.\n\n<img width=\"800\" src=\"https://docs.databricks.com/_static/images/mlflow/quickstart/compare-runs.png\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70e02a64-6878-4b9b-9297-5390c9e19ddc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Load models\nYou can also access the results for a specific run using the MLflow API. The code in the following cell illustrates how to load the model trained in a given MLflow run and use it to make predictions. You can also find code snippets for loading specific models on the MLflow run page ([AWS](https://docs.databricks.com/applications/mlflow/tracking.html#view-notebook-experiment)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/tracking#view-notebook-experiment))."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78c381a0-15ef-4b8c-92a3-73261bf7160f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# After a model has been logged, you can load it in different notebooks or jobs\n# mlflow.pyfunc.load_model makes model prediction available under a common API\nmodel_loaded = mlflow.pyfunc.load_model(\n  'runs:/{run_id}/model'.format(\n    run_id=run.info.run_id\n  )\n)\n\npredictions_loaded = model_loaded.predict(X_test)\npredictions_original = model_2.predict(X_test)\n\n# The loaded model should match the original\nassert(np.array_equal(predictions_loaded, predictions_original))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d85461a-9f6d-43e4-abb5-0d72e1fe48a7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Part 2. Hyperparameter Tuning\nAt this point, you have trained a simple model and used the MLflow tracking service to organize your work. Next, you can perform more sophisticated tuning using Hyperopt."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b6929a8-4d44-4457-a4b3-9ed0e3d3fe2c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Parallel training with Hyperopt and SparkTrials\n[Hyperopt](http://hyperopt.github.io/hyperopt/) is a Python library for hyperparameter tuning. For more information about using Hyperopt in Databricks, see the documentation ([AWS](https://docs.databricks.com/applications/machine-learning/automl-hyperparam-tuning/index.html#hyperparameter-tuning-with-hyperopt)|[Azure](https://docs.microsoft.com/azure/databricks/applications/machine-learning/automl-hyperparam-tuning/index#hyperparameter-tuning-with-hyperopt)).\n\nYou can use Hyperopt with SparkTrials to run hyperparameter sweeps and train multiple models in parallel. This reduces the time required to optimize model performance. MLflow tracking is integrated with Hyperopt to automatically log models and parameters."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"298af1ff-2edc-46b9-ad38-5abbcd303128","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Define the search space to explore\nsearch_space = {\n  'n_estimators': scope.int(hp.quniform('n_estimators', 20, 1000, 1)),\n  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n  'max_depth': scope.int(hp.quniform('max_depth', 2, 5, 1)),\n}\n\ndef train_model(params):\n  # Enable autologging on each worker\n  mlflow.autolog()\n  with mlflow.start_run(nested=True):\n    model_hp = sklearn.ensemble.GradientBoostingClassifier(\n      random_state=0,\n      **params\n    )\n    model_hp.fit(X_train, y_train)\n    predicted_probs = model_hp.predict_proba(X_test)\n    # Tune based on the test AUC\n    # In production, you could use a separate validation set instead\n    roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n    mlflow.log_metric('test_auc', roc_auc)\n    \n    # Set the loss to -1*auc_score so fmin maximizes the auc_score\n    return {'status': STATUS_OK, 'loss': -1*roc_auc}\n\n# SparkTrials distributes the tuning using Spark workers\n# Greater parallelism speeds processing, but each hyperparameter trial has less information from other trials\n# On smaller clusters or Databricks Community Edition try setting parallelism=2\nspark_trials = SparkTrials(\n  parallelism=1\n)\n\nwith mlflow.start_run(run_name='gb_hyperopt') as run:\n  # Use hyperopt to find the parameters yielding the highest AUC\n  best_params = fmin(\n    fn=train_model, \n    space=search_space, \n    algo=tpe.suggest, \n    max_evals=32,\n    trials=spark_trials)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d3f22d5-8217-42ad-bd78-fbf30ef5538c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Search runs to retrieve the best model\nBecause all of the runs are tracked by MLflow, you can retrieve the metrics and parameters for the best run using the MLflow search runs API to find the tuning run with the highest test auc.\n\nThis tuned model should perform better than the simpler models trained in Part 1."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bae7ad39-0eea-4dcd-a58a-dcfc723ba91f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Sort runs by their test auc; in case of ties, use the most recent run\nbest_run = mlflow.search_runs(\n  order_by=['metrics.test_auc DESC', 'start_time DESC'],\n  max_results=10,\n).iloc[0]\nprint('Best Run')\nprint('AUC: {}'.format(best_run[\"metrics.test_auc\"]))\nprint('Num Estimators: {}'.format(best_run[\"params.n_estimators\"]))\nprint('Max Depth: {}'.format(best_run[\"params.max_depth\"]))\nprint('Learning Rate: {}'.format(best_run[\"params.learning_rate\"]))\n\nbest_model_pyfunc = mlflow.pyfunc.load_model(\n  'runs:/{run_id}/model'.format(\n    run_id=best_run.run_id\n  )\n)\n\n#make a dataset with all predictions\nbest_model_predictions = X_test\nbest_model_predictions[\"prediction\"] = best_model_pyfunc.predict(X_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ea75b75-0439-4fa2-9f8c-6bbd2b586719","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Compare multiple runs in the UI\nAs in Part 1, you can view and compare the runs in the MLflow experiment details page, accessible via the external link icon at the top of the **Experiment** sidebar. \n\nOn the experiment details page, click the \"+\" icon to expand the parent run, then select all runs except the parent, and click **Compare**. You can visualize the different runs using a parallel coordinates plot, which shows the impact of different parameter values on a metric. \n\n<img width=\"800\" src=\"https://docs.databricks.com/_static/images/mlflow/quickstart/parallel-plot.png\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32be40f4-ba23-4512-817e-ab62e9a85825","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["results = spark.createDataFrame(best_model_predictions)\nspark.sql(\"drop table if exists ml.default.predictions\")\n\n#Write results back to Unity Catalog from Python\nresults.write.saveAsTable(\"ml.default.predictions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"495ba9dc-11a3-47d2-8920-87928ea9127a","inputWidgets":{},"title":"Write results back to Unity Catalog"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nGRANT SELECT ON ml.default.predictions TO `account users`;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"adaf7f0a-3529-4814-97a0-902d4d06abbc","inputWidgets":{},"title":"Share table with other users"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML All In One","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3832033296203680}},"nbformat":4,"nbformat_minor":0}
